{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kereas Learning\n",
    "Here I will work on utilizing some basics in kereas but on my data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"/Users/victoria/Documents/me/Creatify/python_scripts/playlists 0-999.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>tracks</th>\n",
       "      <th>artists</th>\n",
       "      <th>avg_danceability</th>\n",
       "      <th>avg_energy</th>\n",
       "      <th>avg_loudness</th>\n",
       "      <th>avg_acousticness</th>\n",
       "      <th>avg_instrumentalness</th>\n",
       "      <th>avg_liveness</th>\n",
       "      <th>avg_valence</th>\n",
       "      <th>avg_tempo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Throwbacks</td>\n",
       "      <td>0UaMYEvWZi0ZqiDOoHU3YI 6I9VzXrHxO9rA9A5euc8Ak ...</td>\n",
       "      <td>2wIVse2owClT7go1WT98tk 26dSoYclwsYLMAKD3tpOr4 ...</td>\n",
       "      <td>0.664077</td>\n",
       "      <td>0.781077</td>\n",
       "      <td>-4.891212</td>\n",
       "      <td>0.083674</td>\n",
       "      <td>0.000674</td>\n",
       "      <td>0.187087</td>\n",
       "      <td>0.642750</td>\n",
       "      <td>121.157500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Awesome Playlist</td>\n",
       "      <td>2HHtWyy5CgaQbC7XSoOb0e 1MYYt7h6amcrauCOoso3Gx ...</td>\n",
       "      <td>26bcq2nyj5GB7uRr558iQg 7zdmbPudNX4SQJXnYIuCTC ...</td>\n",
       "      <td>0.492382</td>\n",
       "      <td>0.695923</td>\n",
       "      <td>-8.107974</td>\n",
       "      <td>0.162227</td>\n",
       "      <td>0.223708</td>\n",
       "      <td>0.179344</td>\n",
       "      <td>0.476667</td>\n",
       "      <td>124.987128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>korean</td>\n",
       "      <td>74tqql9zP6JjF5hjkHHUXp 4erhEGuOGQgjv3p1bccnpn ...</td>\n",
       "      <td>7lXgbtBDcCRbfc5f8FhGUL 2e4G04F77jxVuDYo44TCSm ...</td>\n",
       "      <td>0.671062</td>\n",
       "      <td>0.692953</td>\n",
       "      <td>-4.875594</td>\n",
       "      <td>0.269100</td>\n",
       "      <td>0.000638</td>\n",
       "      <td>0.168894</td>\n",
       "      <td>0.565078</td>\n",
       "      <td>114.595984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mat</td>\n",
       "      <td>4WJ7UMD4i6DOPzyXU5pZSz 1Kzxd1kkjaGX4JZz2CYsXB ...</td>\n",
       "      <td>436sYg6CZhNefQJogaXeK0 436sYg6CZhNefQJogaXeK0 ...</td>\n",
       "      <td>0.514429</td>\n",
       "      <td>0.620902</td>\n",
       "      <td>-9.618754</td>\n",
       "      <td>0.273514</td>\n",
       "      <td>0.203156</td>\n",
       "      <td>0.188278</td>\n",
       "      <td>0.451258</td>\n",
       "      <td>125.547627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>90s</td>\n",
       "      <td>4iCGSi1RonREsPtfEKYj5b 5qqabIl2vWzo9ApSC317sa ...</td>\n",
       "      <td>40Yq4vzPs9VNUrIBG5Jr2i 2DaxqgrOhkeH0fpeiQq2f4 ...</td>\n",
       "      <td>0.576235</td>\n",
       "      <td>0.650418</td>\n",
       "      <td>-7.634529</td>\n",
       "      <td>0.177189</td>\n",
       "      <td>0.081759</td>\n",
       "      <td>0.166524</td>\n",
       "      <td>0.490294</td>\n",
       "      <td>127.725412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>old</td>\n",
       "      <td>3Wq1XLo82SODnXJpYHvnyD 709JP07qJbz95uMOLHYlWu ...</td>\n",
       "      <td>0eUVbl6fC6fVsT7xnkSzVJ 6Tz0QRoe083BcOo2YbG9lV ...</td>\n",
       "      <td>0.597244</td>\n",
       "      <td>0.655049</td>\n",
       "      <td>-8.745244</td>\n",
       "      <td>0.235812</td>\n",
       "      <td>0.021555</td>\n",
       "      <td>0.184354</td>\n",
       "      <td>0.674268</td>\n",
       "      <td>123.098171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>Daze</td>\n",
       "      <td>1wZqJM5FGDEl3FjHDxDyQd 7yyRTcZmCiyzzJlNzGC9Ol ...</td>\n",
       "      <td>2HPaUgqeutzr3jx5a9WyDV 5M0lbkGluOPXLeFjApw8r8 ...</td>\n",
       "      <td>0.661235</td>\n",
       "      <td>0.487700</td>\n",
       "      <td>-8.565294</td>\n",
       "      <td>0.338606</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.157359</td>\n",
       "      <td>0.404865</td>\n",
       "      <td>120.064706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>rap</td>\n",
       "      <td>48zmVAYyyhoWfTDe5pGynC 0srwKuJPH8yBzzFUJMBQM2 ...</td>\n",
       "      <td>1rBxtaN521NYi8Z35G7fUn 5LHRHt1k9lMyONurDHEdrp ...</td>\n",
       "      <td>0.734496</td>\n",
       "      <td>0.690647</td>\n",
       "      <td>-5.989563</td>\n",
       "      <td>0.084931</td>\n",
       "      <td>0.016670</td>\n",
       "      <td>0.203448</td>\n",
       "      <td>0.539570</td>\n",
       "      <td>114.652286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>Country</td>\n",
       "      <td>4FkgULes13bk2eHrsJg4q9 0vFMQi8ZnOM2y8cuReZTZ2 ...</td>\n",
       "      <td>7H6dkUChT5EoOQtUVMg4cN 4xFUf1FHVy696Q1JQZMTRj ...</td>\n",
       "      <td>0.558324</td>\n",
       "      <td>0.660167</td>\n",
       "      <td>-5.622509</td>\n",
       "      <td>0.264800</td>\n",
       "      <td>0.000169</td>\n",
       "      <td>0.145799</td>\n",
       "      <td>0.450880</td>\n",
       "      <td>127.872852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>thinking of you</td>\n",
       "      <td>3rUTBx7gmn1IAAwsrjtnO0 6zYqyOJuyXDOvWJwKkZ8tg ...</td>\n",
       "      <td>6oFs3qk4VepIVFdoD4jmsy 2vm8GdHyrJh2O2MfbQFYG0 ...</td>\n",
       "      <td>0.541955</td>\n",
       "      <td>0.334182</td>\n",
       "      <td>-11.205568</td>\n",
       "      <td>0.614580</td>\n",
       "      <td>0.044998</td>\n",
       "      <td>0.137882</td>\n",
       "      <td>0.301930</td>\n",
       "      <td>110.969682</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 name                                             tracks  \\\n",
       "0          Throwbacks  0UaMYEvWZi0ZqiDOoHU3YI 6I9VzXrHxO9rA9A5euc8Ak ...   \n",
       "1    Awesome Playlist  2HHtWyy5CgaQbC7XSoOb0e 1MYYt7h6amcrauCOoso3Gx ...   \n",
       "2             korean   74tqql9zP6JjF5hjkHHUXp 4erhEGuOGQgjv3p1bccnpn ...   \n",
       "3                 mat  4WJ7UMD4i6DOPzyXU5pZSz 1Kzxd1kkjaGX4JZz2CYsXB ...   \n",
       "4                 90s  4iCGSi1RonREsPtfEKYj5b 5qqabIl2vWzo9ApSC317sa ...   \n",
       "..                ...                                                ...   \n",
       "995               old  3Wq1XLo82SODnXJpYHvnyD 709JP07qJbz95uMOLHYlWu ...   \n",
       "996              Daze  1wZqJM5FGDEl3FjHDxDyQd 7yyRTcZmCiyzzJlNzGC9Ol ...   \n",
       "997               rap  48zmVAYyyhoWfTDe5pGynC 0srwKuJPH8yBzzFUJMBQM2 ...   \n",
       "998           Country  4FkgULes13bk2eHrsJg4q9 0vFMQi8ZnOM2y8cuReZTZ2 ...   \n",
       "999   thinking of you  3rUTBx7gmn1IAAwsrjtnO0 6zYqyOJuyXDOvWJwKkZ8tg ...   \n",
       "\n",
       "                                               artists  avg_danceability  \\\n",
       "0    2wIVse2owClT7go1WT98tk 26dSoYclwsYLMAKD3tpOr4 ...          0.664077   \n",
       "1    26bcq2nyj5GB7uRr558iQg 7zdmbPudNX4SQJXnYIuCTC ...          0.492382   \n",
       "2    7lXgbtBDcCRbfc5f8FhGUL 2e4G04F77jxVuDYo44TCSm ...          0.671062   \n",
       "3    436sYg6CZhNefQJogaXeK0 436sYg6CZhNefQJogaXeK0 ...          0.514429   \n",
       "4    40Yq4vzPs9VNUrIBG5Jr2i 2DaxqgrOhkeH0fpeiQq2f4 ...          0.576235   \n",
       "..                                                 ...               ...   \n",
       "995  0eUVbl6fC6fVsT7xnkSzVJ 6Tz0QRoe083BcOo2YbG9lV ...          0.597244   \n",
       "996  2HPaUgqeutzr3jx5a9WyDV 5M0lbkGluOPXLeFjApw8r8 ...          0.661235   \n",
       "997  1rBxtaN521NYi8Z35G7fUn 5LHRHt1k9lMyONurDHEdrp ...          0.734496   \n",
       "998  7H6dkUChT5EoOQtUVMg4cN 4xFUf1FHVy696Q1JQZMTRj ...          0.558324   \n",
       "999  6oFs3qk4VepIVFdoD4jmsy 2vm8GdHyrJh2O2MfbQFYG0 ...          0.541955   \n",
       "\n",
       "     avg_energy  avg_loudness  avg_acousticness  avg_instrumentalness  \\\n",
       "0      0.781077     -4.891212          0.083674              0.000674   \n",
       "1      0.695923     -8.107974          0.162227              0.223708   \n",
       "2      0.692953     -4.875594          0.269100              0.000638   \n",
       "3      0.620902     -9.618754          0.273514              0.203156   \n",
       "4      0.650418     -7.634529          0.177189              0.081759   \n",
       "..          ...           ...               ...                   ...   \n",
       "995    0.655049     -8.745244          0.235812              0.021555   \n",
       "996    0.487700     -8.565294          0.338606              0.000016   \n",
       "997    0.690647     -5.989563          0.084931              0.016670   \n",
       "998    0.660167     -5.622509          0.264800              0.000169   \n",
       "999    0.334182    -11.205568          0.614580              0.044998   \n",
       "\n",
       "     avg_liveness  avg_valence   avg_tempo  \n",
       "0        0.187087     0.642750  121.157500  \n",
       "1        0.179344     0.476667  124.987128  \n",
       "2        0.168894     0.565078  114.595984  \n",
       "3        0.188278     0.451258  125.547627  \n",
       "4        0.166524     0.490294  127.725412  \n",
       "..            ...          ...         ...  \n",
       "995      0.184354     0.674268  123.098171  \n",
       "996      0.157359     0.404865  120.064706  \n",
       "997      0.203448     0.539570  114.652286  \n",
       "998      0.145799     0.450880  127.872852  \n",
       "999      0.137882     0.301930  110.969682  \n",
       "\n",
       "[1000 rows x 11 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "- Tokenization of string data, followed by token indexing.\n",
    "- Feature normalization.\n",
    "- Rescaling the data to small values (in general, input values to a neural network should be close to zero -- typically we expect either data with zero-mean and unit-variance, or data in the [0, 1] range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-08 11:38:20.295210: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# Tokenization of strin data\n",
    "from tensorflow.keras.layers import TextVectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = data['name'].tolist()\n",
    "training_data = np.array(names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Turning strings into sequences of integer word indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[ 65   0   0 ...   0   0   0]\n",
      " [ 81  11   0 ...   0   0   0]\n",
      " [137   0   0 ...   0   0   0]\n",
      " ...\n",
      " [ 15   0   0 ...   0   0   0]\n",
      " [  2   0   0 ...   0   0   0]\n",
      " [111  30  20 ...   0   0   0]], shape=(1000, 8), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "# Create a TextVectorization layer instance. It can be configured to either\n",
    "# return integer token indices, or a dense token representation (e.g. multi-hot\n",
    "# or TF-IDF). The text standardization and text splitting algorithms are fully\n",
    "# configurable.\n",
    "vectorizer = TextVectorization(output_mode=\"int\")\n",
    "\n",
    "# Calling `adapt` on an array or dataset makes the layer generate a vocabulary\n",
    "# index for the data, which can then be reused when seeing new data.\n",
    "vectorizer.adapt(training_data)\n",
    "\n",
    "# After calling adapt, the layer is able to encode any n-gram it has seen before\n",
    "# in the `adapt()` data. Unknown n-grams are encoded via an \"out-of-vocabulary\"\n",
    "# token.\n",
    "integer_data = vectorizer(training_data)\n",
    "print(integer_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalizing Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "var: 1.0005\n",
      "mean: 0.0000\n"
     ]
    }
   ],
   "source": [
    "# Example image data, with values in the [0, 255] range\n",
    "training_data = np.random.randint(0, 256, size=(64, 200, 200, 3)).astype(\"float32\")\n",
    "\n",
    "normalizer = Normalization(axis=-1)\n",
    "normalizer.adapt(training_data)\n",
    "\n",
    "normalized_data = normalizer(training_data)\n",
    "print(\"var: %.4f\" % np.var(normalized_data))\n",
    "print(\"mean: %.4f\" % np.mean(normalized_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building models with the Keras Functional API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define our inputs\n",
    "inputs = keras.Input(shape=(1, 11))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After defining your input(s), you can chain layer transformations on top of your inputs, until your final output:\n",
    "# Apply some convolution and pooling layers\n",
    "\n",
    "# TODO: will need to determien what I want to do here with my layers\n",
    "\n",
    "x = training_data\n",
    "\n",
    "x = layers.Conv2D(filters=32, kernel_size=(3, 3), activation=\"relu\")(x)\n",
    "x = layers.MaxPooling2D(pool_size=(3, 3))(x)\n",
    "x = layers.Conv2D(filters=32, kernel_size=(3, 3), activation=\"relu\")(x)\n",
    "x = layers.MaxPooling2D(pool_size=(3, 3))(x)\n",
    "x = layers.Conv2D(filters=32, kernel_size=(3, 3), activation=\"relu\")(x)\n",
    "\n",
    "# Apply global average pooling to get flat feature vectors\n",
    "x = layers.GlobalAveragePooling2D()(x)\n",
    "\n",
    "# Add a dense classifier on top\n",
    "num_classes = 10\n",
    "outputs = layers.Dense(num_classes, activation=\"softmax\")(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Once you have defined the directed acyclic graph of layers that turns your input(s) into your outputs, instantiate a Model object\n",
    "model = keras.Model(inputs=training_data, outputs=outputs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A New Tutorial\n",
    "Okay that was somewhat helpful... Let's work on a new tutorial found at https://machinelearningmastery.com/tutorial-first-neural-network-python-keras/ to see what else I can elearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import loadtxt\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset\n",
    "dataset = loadtxt('pima-indians-diabetes.csv', delimiter=',')\n",
    "# split into input (X) and output (y) variables\n",
    "X = dataset[:,0:8]\n",
    "y = dataset[:,8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define A Model\n",
    "Models in Keras are defined as a sequence of layers.\n",
    "\n",
    "We create a Sequential model and add layers one at a time until we are happy with our network architecture.\n",
    "\n",
    "\n",
    "The first thing to get right is to ensure the input layer has the correct number of input features. This can be specified when creating the first layer with the input_shape argument and setting it to (8,) for presenting the eight input variables as a vector.\n",
    "\n",
    "\n",
    "In this example, let’s use a fully-connected network structure with three layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fully connected layers are defined using the Dense class. You can specify the number of neurons or nodes in the layer as the first argument and the activation function using the activation argument.\n",
    "\n",
    "Also, you will use the rectified linear unit activation function referred to as ReLU on the first two layers and the Sigmoid function in the output layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### You can piece it all together by adding each layer:\n",
    "\n",
    "- The model expects rows of data with 8 variables (the input_shape=(8,) argument).\n",
    "- The first hidden layer has 12 nodes and uses the relu activation function.\n",
    "- The second hidden layer has 8 nodes and uses the relu activation function.\n",
    "- The output layer has one node and uses the sigmoid activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(12, input_shape=(8,), activation='relu'))\n",
    "model.add(Dense(8, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You must specify the loss function to use to evaluate a set of weights, the optimizer used to search through different weights for the network, and any optional metrics you want to collect and report during training.\n",
    "\n",
    "In this case, use cross entropy as the loss argument. This loss is for a binary classification problems and is defined in Keras as “binary_crossentropy“. You can learn more about choosing loss functions based on your problem here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The adam optimizer is a popular for of gradient descent\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These configurations can be chosen experimentally by trial and error. You want to train the model enough so that it learns a good (or good enough) mapping of rows of input data to the output classification. The model will always have some error, but the amount of error will level out after some point for a given model configuration. This is called model convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "77/77 [==============================] - 1s 2ms/step - loss: 4.0695 - accuracy: 0.5690\n",
      "Epoch 2/150\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 1.1019 - accuracy: 0.5664\n",
      "Epoch 3/150\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.9470 - accuracy: 0.5833\n",
      "Epoch 4/150\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.8155 - accuracy: 0.6224\n",
      "Epoch 5/150\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.7540 - accuracy: 0.6302\n",
      "Epoch 6/150\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.7666 - accuracy: 0.6276\n",
      "Epoch 7/150\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.7124 - accuracy: 0.6172\n",
      "Epoch 8/150\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.6946 - accuracy: 0.6510\n",
      "Epoch 9/150\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.6601 - accuracy: 0.6589\n",
      "Epoch 10/150\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.6654 - accuracy: 0.6484\n",
      "Epoch 11/150\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.6500 - accuracy: 0.6536\n",
      "Epoch 12/150\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.6307 - accuracy: 0.6849\n",
      "Epoch 13/150\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.6297 - accuracy: 0.6732\n",
      "Epoch 14/150\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.6249 - accuracy: 0.6758\n",
      "Epoch 15/150\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.6579 - accuracy: 0.6549\n",
      "Epoch 16/150\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.6204 - accuracy: 0.6628\n",
      "Epoch 17/150\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.6471 - accuracy: 0.6693\n",
      "Epoch 18/150\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.6139 - accuracy: 0.6901\n",
      "Epoch 19/150\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.6168 - accuracy: 0.6979\n",
      "Epoch 20/150\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.6139 - accuracy: 0.6810\n",
      "Epoch 21/150\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.5948 - accuracy: 0.6862\n",
      "Epoch 22/150\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.6007 - accuracy: 0.6849\n",
      "Epoch 23/150\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.6076 - accuracy: 0.6901\n",
      "Epoch 24/150\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.6310 - accuracy: 0.6667\n",
      "Epoch 25/150\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5822 - accuracy: 0.7031\n",
      "Epoch 26/150\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5991 - accuracy: 0.6823\n",
      "Epoch 27/150\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.6050 - accuracy: 0.6758\n",
      "Epoch 28/150\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.6052 - accuracy: 0.6862\n",
      "Epoch 29/150\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5927 - accuracy: 0.7031\n",
      "Epoch 30/150\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5667 - accuracy: 0.7253\n",
      "Epoch 31/150\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.6025 - accuracy: 0.7031\n",
      "Epoch 32/150\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5784 - accuracy: 0.6914\n",
      "Epoch 33/150\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5808 - accuracy: 0.7070\n",
      "Epoch 34/150\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5853 - accuracy: 0.7031\n",
      "Epoch 35/150\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.6196 - accuracy: 0.6745\n",
      "Epoch 36/150\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5748 - accuracy: 0.6979\n",
      "Epoch 37/150\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5742 - accuracy: 0.7135\n",
      "Epoch 38/150\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5665 - accuracy: 0.7031\n",
      "Epoch 39/150\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5505 - accuracy: 0.7357\n",
      "Epoch 40/150\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.6017 - accuracy: 0.6979\n",
      "Epoch 41/150\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5709 - accuracy: 0.7148\n",
      "Epoch 42/150\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5591 - accuracy: 0.7188\n",
      "Epoch 43/150\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5704 - accuracy: 0.7135\n",
      "Epoch 44/150\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5536 - accuracy: 0.7279\n",
      "Epoch 45/150\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5803 - accuracy: 0.6953\n",
      "Epoch 46/150\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5610 - accuracy: 0.7214\n",
      "Epoch 47/150\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5611 - accuracy: 0.7161\n",
      "Epoch 48/150\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5759 - accuracy: 0.7018\n",
      "Epoch 49/150\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5738 - accuracy: 0.6940\n",
      "Epoch 50/150\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5747 - accuracy: 0.7201\n",
      "Epoch 51/150\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5489 - accuracy: 0.7201\n",
      "Epoch 52/150\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5359 - accuracy: 0.7227\n",
      "Epoch 53/150\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5470 - accuracy: 0.7227\n",
      "Epoch 54/150\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5460 - accuracy: 0.7135\n",
      "Epoch 55/150\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5713 - accuracy: 0.7122\n",
      "Epoch 56/150\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5384 - accuracy: 0.7266\n",
      "Epoch 57/150\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5394 - accuracy: 0.7253\n",
      "Epoch 58/150\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5598 - accuracy: 0.7188\n",
      "Epoch 59/150\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5355 - accuracy: 0.7161\n",
      "Epoch 60/150\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5407 - accuracy: 0.7318\n",
      "Epoch 61/150\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.5524 - accuracy: 0.7240\n",
      "Epoch 62/150\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5316 - accuracy: 0.7370\n",
      "Epoch 63/150\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5331 - accuracy: 0.7227\n",
      "Epoch 64/150\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.5278 - accuracy: 0.7214\n",
      "Epoch 65/150\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.5484 - accuracy: 0.7201\n",
      "Epoch 66/150\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5513 - accuracy: 0.7214\n",
      "Epoch 67/150\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5511 - accuracy: 0.7279\n",
      "Epoch 68/150\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5314 - accuracy: 0.7370\n",
      "Epoch 69/150\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5525 - accuracy: 0.7174\n",
      "Epoch 70/150\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5276 - accuracy: 0.7201\n",
      "Epoch 71/150\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5707 - accuracy: 0.7201\n",
      "Epoch 72/150\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5442 - accuracy: 0.7266\n",
      "Epoch 73/150\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5208 - accuracy: 0.7474\n",
      "Epoch 74/150\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5328 - accuracy: 0.7279\n",
      "Epoch 75/150\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5693 - accuracy: 0.7188\n",
      "Epoch 76/150\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.5202 - accuracy: 0.7435\n",
      "Epoch 77/150\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5144 - accuracy: 0.7448\n",
      "Epoch 78/150\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5200 - accuracy: 0.7448\n",
      "Epoch 79/150\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5044 - accuracy: 0.7422\n",
      "Epoch 80/150\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.5350 - accuracy: 0.7331\n",
      "Epoch 81/150\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5277 - accuracy: 0.7344\n",
      "Epoch 82/150\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5036 - accuracy: 0.7591\n",
      "Epoch 83/150\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5078 - accuracy: 0.7578\n",
      "Epoch 84/150\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5304 - accuracy: 0.7461\n",
      "Epoch 85/150\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5142 - accuracy: 0.7500\n",
      "Epoch 86/150\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5139 - accuracy: 0.7526\n",
      "Epoch 87/150\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5069 - accuracy: 0.7591\n",
      "Epoch 88/150\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5212 - accuracy: 0.7448\n",
      "Epoch 89/150\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5315 - accuracy: 0.7357\n",
      "Epoch 90/150\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5111 - accuracy: 0.7422\n",
      "Epoch 91/150\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5121 - accuracy: 0.7474\n",
      "Epoch 92/150\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.4993 - accuracy: 0.7409\n",
      "Epoch 93/150\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5041 - accuracy: 0.7474\n",
      "Epoch 94/150\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.4920 - accuracy: 0.7565\n",
      "Epoch 95/150\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5012 - accuracy: 0.7539\n",
      "Epoch 96/150\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5095 - accuracy: 0.7552\n",
      "Epoch 97/150\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5086 - accuracy: 0.7422\n",
      "Epoch 98/150\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5283 - accuracy: 0.7370\n",
      "Epoch 99/150\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5084 - accuracy: 0.7513\n",
      "Epoch 100/150\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.4957 - accuracy: 0.7591\n",
      "Epoch 101/150\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5200 - accuracy: 0.7500\n",
      "Epoch 102/150\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5098 - accuracy: 0.7682\n",
      "Epoch 103/150\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.4913 - accuracy: 0.7487\n",
      "Epoch 104/150\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5023 - accuracy: 0.7474\n",
      "Epoch 105/150\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5024 - accuracy: 0.7526\n",
      "Epoch 106/150\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.4996 - accuracy: 0.7578\n",
      "Epoch 107/150\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.5096 - accuracy: 0.7344\n",
      "Epoch 108/150\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.4975 - accuracy: 0.7526\n",
      "Epoch 109/150\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.4902 - accuracy: 0.7513\n",
      "Epoch 110/150\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5045 - accuracy: 0.7617\n",
      "Epoch 111/150\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5129 - accuracy: 0.7617\n",
      "Epoch 112/150\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5011 - accuracy: 0.7565\n",
      "Epoch 113/150\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.4930 - accuracy: 0.7539\n",
      "Epoch 114/150\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.4966 - accuracy: 0.7474\n",
      "Epoch 115/150\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.4823 - accuracy: 0.7734\n",
      "Epoch 116/150\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.4903 - accuracy: 0.7721\n",
      "Epoch 117/150\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.4846 - accuracy: 0.7604\n",
      "Epoch 118/150\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5031 - accuracy: 0.7591\n",
      "Epoch 119/150\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.4887 - accuracy: 0.7487\n",
      "Epoch 120/150\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.4854 - accuracy: 0.7604\n",
      "Epoch 121/150\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.4789 - accuracy: 0.7695\n",
      "Epoch 122/150\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5079 - accuracy: 0.7448\n",
      "Epoch 123/150\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5007 - accuracy: 0.7591\n",
      "Epoch 124/150\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.4837 - accuracy: 0.7604\n",
      "Epoch 125/150\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.4884 - accuracy: 0.7760\n",
      "Epoch 126/150\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5029 - accuracy: 0.7708\n",
      "Epoch 127/150\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.4904 - accuracy: 0.7448\n",
      "Epoch 128/150\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.4775 - accuracy: 0.7773\n",
      "Epoch 129/150\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5016 - accuracy: 0.7435\n",
      "Epoch 130/150\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.4767 - accuracy: 0.7708\n",
      "Epoch 131/150\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.4940 - accuracy: 0.7461\n",
      "Epoch 132/150\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.4661 - accuracy: 0.7695\n",
      "Epoch 133/150\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.4907 - accuracy: 0.7474\n",
      "Epoch 134/150\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.4792 - accuracy: 0.7604\n",
      "Epoch 135/150\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.4897 - accuracy: 0.7552\n",
      "Epoch 136/150\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.4825 - accuracy: 0.7708\n",
      "Epoch 137/150\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.4749 - accuracy: 0.7760\n",
      "Epoch 138/150\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.4818 - accuracy: 0.7682\n",
      "Epoch 139/150\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.4986 - accuracy: 0.7500\n",
      "Epoch 140/150\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.4743 - accuracy: 0.7669\n",
      "Epoch 141/150\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.4778 - accuracy: 0.7617\n",
      "Epoch 142/150\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.4836 - accuracy: 0.7682\n",
      "Epoch 143/150\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.4802 - accuracy: 0.7487\n",
      "Epoch 144/150\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.4665 - accuracy: 0.7708\n",
      "Epoch 145/150\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.4957 - accuracy: 0.7487\n",
      "Epoch 146/150\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.4839 - accuracy: 0.7682\n",
      "Epoch 147/150\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.4718 - accuracy: 0.7695\n",
      "Epoch 148/150\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.4818 - accuracy: 0.7721\n",
      "Epoch 149/150\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.4811 - accuracy: 0.7682\n",
      "Epoch 150/150\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.4912 - accuracy: 0.7487\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fad870076d0>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit the keras model on the dataset\n",
    "# This problem will run for a small number of epochs (150) and use a relatively small batch size of 10.\n",
    "model.fit(X, y, epochs=150, batch_size=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 76.43\n"
     ]
    }
   ],
   "source": [
    "# evaluate the keras model\n",
    "_, accuracy = model.evaluate(X, y, verbose=0)\n",
    "print('Accuracy: %.2f' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/24 [==============================] - 0s 1ms/step\n"
     ]
    }
   ],
   "source": [
    "# make probability predictions with the model\n",
    "predictions = model.predict(X)\n",
    "# round predictions \n",
    "rounded = [round(x[0]) for x in predictions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/24 [==============================] - 0s 1ms/step\n"
     ]
    }
   ],
   "source": [
    "# make class predictions with the model\n",
    "predictions = (model.predict(X) > 0.5).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "74a37a82a1045f722fa39b5e25843ead82fe26f95a8a3d59b6a29879869666f7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
