{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing a Neural Network\n",
    "Using the data in the Million Playlist Dataset, I hope to train a neural network to predict good names for a playlist. For now, I will only work with the first file but will later expand to them all (as a test train split)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gathering the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1 = open('/Users/victoria/Documents/me/Creatify/notebooks/data/mpd.slice.0-999.json')\n",
    "data = json.load(f1)\n",
    "\n",
    "# Gather a list of playlists names\n",
    "playlist_names = []\n",
    "for i in range(len(data['playlists'])):\n",
    "    playlist_names.append(data['playlists'][i]['name'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Starting HERE everything is written for one playlist. Eventually, it will have a larger scope and be within a loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spotify:track:0UaMYEvWZi0ZqiDOoHU3YI\n",
      "0UaMYEvWZi0ZqiDOoHU3YI\n",
      "spotify:track:6I9VzXrHxO9rA9A5euc8Ak\n",
      "6I9VzXrHxO9rA9A5euc8Ak\n",
      "spotify:track:0WqIKmW4BTrj3eJFmnCKMv\n",
      "0WqIKmW4BTrj3eJFmnCKMv\n",
      "spotify:track:1AWQoqb9bSvzTjaLralEkT\n",
      "1AWQoqb9bSvzTjaLralEkT\n",
      "spotify:track:1lzr43nnXAijIGYnCT8M8H\n",
      "1lzr43nnXAijIGYnCT8M8H\n",
      "spotify:track:0XUfyU2QviPAs6bxSpXYG4\n",
      "0XUfyU2QviPAs6bxSpXYG4\n",
      "spotify:track:68vgtRHr7iZHpzGpon6Jlo\n",
      "68vgtRHr7iZHpzGpon6Jlo\n",
      "spotify:track:3BxWKCI06eQ5Od8TY2JBeA\n",
      "3BxWKCI06eQ5Od8TY2JBeA\n",
      "spotify:track:7H6ev70Weq6DdpZyyTmUXk\n",
      "7H6ev70Weq6DdpZyyTmUXk\n",
      "spotify:track:2PpruBYCo4H7WOBJ7Q2EwM\n",
      "2PpruBYCo4H7WOBJ7Q2EwM\n",
      "spotify:track:2gam98EZKrF9XuOkU13ApN\n",
      "2gam98EZKrF9XuOkU13ApN\n",
      "spotify:track:4Y45aqo9QMa57rDsAJv40A\n",
      "4Y45aqo9QMa57rDsAJv40A\n",
      "spotify:track:1HwpWwa6bnqqRhK8agG4RS\n",
      "1HwpWwa6bnqqRhK8agG4RS\n",
      "spotify:track:20ORwCJusz4KS2PbTPVNKo\n",
      "20ORwCJusz4KS2PbTPVNKo\n",
      "spotify:track:7k6IzwMGpxnRghE7YosnXT\n",
      "7k6IzwMGpxnRghE7YosnXT\n",
      "spotify:track:1Bv0Yl01xBDZD4OQP93fyl\n",
      "1Bv0Yl01xBDZD4OQP93fyl\n",
      "spotify:track:4omisSlTk6Dsq2iQD7MA07\n",
      "4omisSlTk6Dsq2iQD7MA07\n",
      "spotify:track:7xYnUQigPoIDAMPVK79NEq\n",
      "7xYnUQigPoIDAMPVK79NEq\n",
      "spotify:track:6d8A5sAx9TfdeseDvfWNHd\n",
      "6d8A5sAx9TfdeseDvfWNHd\n",
      "spotify:track:4pmc2AxSEq6g7hPVlJCPyP\n",
      "4pmc2AxSEq6g7hPVlJCPyP\n",
      "spotify:track:215JYyyUnrJ98NK3KEwu6d\n",
      "215JYyyUnrJ98NK3KEwu6d\n",
      "spotify:track:0uqPG793dkDDN7sCUJJIVC\n",
      "0uqPG793dkDDN7sCUJJIVC\n",
      "spotify:track:19Js5ypV6JKn4DMExHQbGc\n",
      "19Js5ypV6JKn4DMExHQbGc\n",
      "spotify:track:1JURww012QnWAw0zZXi6Aa\n",
      "1JURww012QnWAw0zZXi6Aa\n",
      "spotify:track:7DFnq8FYhHMCylykf6ZCxA\n",
      "7DFnq8FYhHMCylykf6ZCxA\n",
      "spotify:track:1TfAhjzRBWzYZ8IdUV3igl\n",
      "1TfAhjzRBWzYZ8IdUV3igl\n",
      "spotify:track:1Y4ZdPOOgCUhBcKZOrUFiS\n",
      "1Y4ZdPOOgCUhBcKZOrUFiS\n",
      "spotify:track:6MjljecHzHelUDismyKkba\n",
      "6MjljecHzHelUDismyKkba\n",
      "spotify:track:67T6l4q3zVjC5nZZPXByU8\n",
      "67T6l4q3zVjC5nZZPXByU8\n",
      "spotify:track:34ceTg8ChN5HjrqiIYCn9Q\n",
      "34ceTg8ChN5HjrqiIYCn9Q\n",
      "spotify:track:5Q0Nhxo0l2bP3pNjpGJwV1\n",
      "5Q0Nhxo0l2bP3pNjpGJwV1\n",
      "spotify:track:6GIrIt2M39wEGwjCQjGChX\n",
      "6GIrIt2M39wEGwjCQjGChX\n",
      "spotify:track:4E5P1XyAFtrjpiIxkydly4\n",
      "4E5P1XyAFtrjpiIxkydly4\n",
      "spotify:track:3H1LCvO3fVsK2HPguhbml0\n",
      "3H1LCvO3fVsK2HPguhbml0\n",
      "spotify:track:3uoQULcUWfnt6nc6J7Vgai\n",
      "3uoQULcUWfnt6nc6J7Vgai\n",
      "spotify:track:2nbClS09zsIAqNkshg6jnp\n",
      "2nbClS09zsIAqNkshg6jnp\n",
      "spotify:track:69ghzc538EQSVon2Gm3wrr\n",
      "69ghzc538EQSVon2Gm3wrr\n",
      "spotify:track:1kusepF3AacIEtUTYrw4GV\n",
      "1kusepF3AacIEtUTYrw4GV\n",
      "spotify:track:7oK9VyNzrYvRFo7nQEYkWN\n",
      "7oK9VyNzrYvRFo7nQEYkWN\n",
      "spotify:track:12qZHAeOyTf93YAWvGDTat\n",
      "12qZHAeOyTf93YAWvGDTat\n",
      "spotify:track:2jFlMILIQzs7lSFudG9lbo\n",
      "2jFlMILIQzs7lSFudG9lbo\n",
      "spotify:track:4I2GqMe7L2ccMpUbnDzYLH\n",
      "4I2GqMe7L2ccMpUbnDzYLH\n",
      "spotify:track:5lDriBxJd22IhOH9zTcFrV\n",
      "5lDriBxJd22IhOH9zTcFrV\n",
      "spotify:track:2eJ8ij1T3cNUKiGdcUvKhy\n",
      "2eJ8ij1T3cNUKiGdcUvKhy\n",
      "spotify:track:5y69gQtK33qxb8a24ACkCy\n",
      "5y69gQtK33qxb8a24ACkCy\n",
      "spotify:track:1X5WGCrUMuwRFuYU1eAo2I\n",
      "1X5WGCrUMuwRFuYU1eAo2I\n",
      "spotify:track:3utIAb67sOu0QHxBE88P1M\n",
      "3utIAb67sOu0QHxBE88P1M\n",
      "spotify:track:3jkdQNkDTxxXtjSO4l0o1H\n",
      "3jkdQNkDTxxXtjSO4l0o1H\n",
      "spotify:track:5c1sfI6wIQEsSUw0xrkFdl\n",
      "5c1sfI6wIQEsSUw0xrkFdl\n",
      "spotify:track:6sqNctd7MlJoKDOxPVCAvU\n",
      "6sqNctd7MlJoKDOxPVCAvU\n",
      "spotify:track:1b7vg5T9YKR3NNqXfBYRF7\n",
      "1b7vg5T9YKR3NNqXfBYRF7\n",
      "spotify:track:6GIrIt2M39wEGwjCQjGChX\n",
      "6GIrIt2M39wEGwjCQjGChX\n"
     ]
    }
   ],
   "source": [
    "playlist_tracks = []\n",
    "for i in range(len(data['playlists'][0]['tracks'])):\n",
    "    song_id = data['playlists'][0]['tracks'][i]['track_uri']\n",
    "    print(song_id)\n",
    "    song_id = song_id.replace(\"spotify:track:\", \"\")\n",
    "    print(song_id)\n",
    "    playlist_tracks.append(song_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the artisits\n",
    "playlist_artists = []\n",
    "for i in range(len(data['playlists'][0]['tracks'])):\n",
    "    artist_id = data['playlists'][0]['tracks'][i]['artist_uri']\n",
    "    artist_id = artist_id.replace(\"spotify:artist:\", \"\")\n",
    "    playlist_artists.append(artist_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Psycho Acoustic Analyzation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotipy import oauth2\n",
    "import re\n",
    "import credentials\n",
    "import spotipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1 = open('data/mpd.slice.0-999.json')\n",
    "data = json.load(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/pz/lc5f5cgd1m5_ghmjddlz8r600000gn/T/ipykernel_30174/2781803620.py:7: DeprecationWarning: You're using 'as_dict = True'.get_access_token will return the token string directly in future versions. Please adjust your code accordingly, or use get_cached_token instead.\n",
      "  token = sp_oauth.get_access_token(code)\n"
     ]
    }
   ],
   "source": [
    "# Gain access to mainupation\n",
    "SCOPE = ('user-read-recently-played,user-library-read,user-read-currently-playing,playlist-read-private,playlist-modify-private,playlist-modify-public,user-read-email,user-modify-playback-state,user-read-private,user-read-playback-state')\n",
    "sp_oauth = oauth2.SpotifyOAuth(credentials.SPOTIPY_CLIENT_ID,credentials.SPOTIPY_CLIENT_SECRET, credentials.SPOTIPY_REDIRECT_URI ,scope=SCOPE )\n",
    "\n",
    "#click \"Accept\" in your browser when the auth window pops up\n",
    "code = sp_oauth.get_auth_response(open_browser=True)\n",
    "token = sp_oauth.get_access_token(code)\n",
    "refresh_token = token['refresh_token']\n",
    "sp = spotipy.Spotify(auth=token['access_token'])\n",
    "username = sp.current_user()['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_playlist(tracks):\n",
    "    \n",
    "    # Create empty dataframe\n",
    "    playlist_features_list = [\"danceability\",\"energy\",\"loudness\", \"speechiness\",\"instrumentalness\",\"liveness\"]\n",
    "    \n",
    "    playlist_df = pd.DataFrame(columns = playlist_features_list)\n",
    "    \n",
    "    # Loop through every track in the playlist, extract features and append the features to the playlist df\n",
    "    for track in tracks:\n",
    "        # Get audio features\n",
    "        audio_features = sp.audio_features(track)\n",
    "        \n",
    "        # Concat the dfs\n",
    "        track_df = pd.DataFrame(audio_features, index=[0])\n",
    "        partion =  track_df[playlist_features_list]\n",
    "        playlist_df = pd.concat([playlist_df, partion], axis = 0, ignore_index = True)\n",
    "        \n",
    "    return playlist_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the playlist\n",
    "playlist_0 = analyze_playlist(playlist_tracks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Emotional Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lyricsgenius\n",
    "import nltk\n",
    "import requests\n",
    "genius = lyricsgenius.Genius('DvaG41Y_l2PyiMbHK8tpKiNzZYZ905GrEo7TTwFCDtqxf7C55K7s36IdKy3AYD9M', verbose=False, retries = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"throwback\", \"throwbacks\", \"throwback\", \"classic\", \"80s\", \"old_school\", \"throwbacks\", \"90s\", \"modern_twist\", \"iconic\", \"retro\", \"Throwback\", \"retro\", \"nice_throwback\", \"90\", \"harkens\", \"iconic\", \"90s\", \"80s\", \"2000s\", \"80s\", \"harken\", \"reminiscent\", \"Reminiscent\", \"80\", \"90s\", \"modern_take\", \"nostalgic\", \"90s\", \"classic\", \"harkening\", \"homage\", \"favorite_era\", \"stylings\", \"early_90s\", \"2000s\", \"reminiscent\", \"golden_era\", \"old-school\", \"80\", \"oldschool\", \"hearkens\", \"old_style\", \"esque\", \"heyday\", \"80s\", \"pretty_dope\", \"mid_90s\", \"simpler_time\", \"90\", \"80\", \"70s\", \"late_80s\", \"themed\", \"retro_style\", \"classic_feel\", \"best_era\", \"nineties\", \"late_80s/early_90s\", \"1990s\", \"modern_version\", \"eighties\", \"classic_look\", \"nostalgia\", \"2010s\", \"way_cooler\", \"90s\", \"themed\", \"different_era\", \"classic\", \"2010s\", \"25th_anniversary\", \"esque\", \"late_90s\", \"80s/90s\", \"the_early_2000s\", \"00s\", \"2000s\", \"golden_days\", \"80's/90\", \"reminds\", \"nice_nod\", \"90\", \"00s\", \"hay_day\", \"90s\", \"80s\", \"60\"]\n",
      "[\"party\", \"pregame\", \"pre-game\", \"pre_game\", \"party\", \"pregame\", \"intermission\", \"ball_game\", \"football_game\", \"random_party\", \"afterparty\", \"soccer_game\", \"beer_pong\", \"basketball_game\", \"Saturday_night\", \"hockey_game\", \"party_bus\", \"Monday_night\", \"weekend\", \"team_meeting\", \"Tuesday_night\", \"invited\", \"ballgame\", \"drinking_game\", \"next_party\", \"dance_party\", \"first_intermission\", \"football_games\", \"super_bowl_party\", \"great_party\", \"green_room\", \"Friday_night\", \"_party\", \"tailgates\", \"superbowl_party\", \"clubhouse\", \"baseball_game\", \"heckle\", \"hockey_games\", \"Wednesday_night\", \"actual_party\", \"Invited\", \"local_pub\", \"block_party\", \"small_party\", \"gameday\", \"game_night\", \"last_nights\", \"saturday_night\", \"Saturday_night\", \"friday_night\", \"Pregame\", \"beer_run\", \"pool_party\", \"pub\", \"Monday_night\", \"watch_party\", \"bar_trivia\", \"drinking_games\", \"pregame_show\", \"hangout\", \"intermissions\", \"Sunday_night\", \"shindig\", \"local_bar\", \"lobby\", \"game_day\", \"cheering\", \"Super_Bowl_party\", \"last_call\", \"social_event\", \"Superbowl_party\", \"last_party\", \"other_night\", \"sports_bar\", \"saturday_night\", \"Thursday_night\", \"party\", \"get-together\", \"festivities\", \"basketball_games\", \"rain_delay\", \"house_party\", \"pep_rally\", \"star_game\", \"Rangers_game\", \"heckling\", \"skills_competition\", \"front_row\", \"sex_party\", \"NYE_party\", \"Thursday_nights\", \"late_night\"]\n",
      "[\"workout\", \"cardio\", \"lift_weights\", \"workout\", \"gym\", \"work_outs\", \"lifting\", \"freestyle\", \"workout\", \"workouts\", \"gym\", \"cardio\", \"calisthenics\", \"aerobics\", \"weight_lifting\", \"weightlifting\", \"spin_class\", \"gym_time\", \"strength_training\", \"good_workout\", \"rap\", \"weights\", \"HIIT\", \"jogging\", \"weight_training\", \"circuit_training\", \"weightlifting\", \"Lift_weights\", \"little_cardio\", \"light_cardio\", \"bodybuilding\", \"workout_playlist\", \"HIIT\", \"body_building\", \"leg_days\", \"heavy_lifts\", \"gym_everyday\", \"routine\", \"great_workout\", \"hard_workout\", \"yoga\", \"whole_workout\", \"bodyweight_exercises\", \"routine\", \"strength_work\", \"heavy_weights\", \"cardio_workout\", \"excercise\", \"gym_day\", \"entire_workout\", \"crossfit\", \"bodybuilding\", \"LISS\", \"bodybuild\", \"hip_hop\", \"more_cardio\", \"groove\", \"spin_classes\", \"cardio_workouts\", \"light_weights\", \"fucking_gym\", \"lifting_sessions\", \"leg_workout\", \"intense_workouts\", \"cardio_work\", \"normal_workout\", \"gym_session\", \"weightlift\", \"party_music\", \"workout_routine\", \"bodyweight_exercise\", \"Trap_music\", \"cardio\", \"freestyling\", \"ab_workouts\", \"body_weight_training\", \"recovery_days\", \"rap\", \"rest_days\", \"low_reps\", \"workout\", \"leg_workouts\", \"freestyle\", \"active_recovery\", \"interval_training\", \"bodyweight_stuff\", \"bodybuilder\"]\n"
     ]
    }
   ],
   "source": [
    "# 2000s, throwback, olides, workout, party, shower\n",
    "# models = fiction, nytimes, and reddit.\n",
    "from empath import Empath\n",
    "lexicon = Empath()\n",
    "lexicon.create_category(\"throwback\", [\"throwback\"],model = \"reddit\")\n",
    "lexicon.create_category(\"party\", [\"party\",\"pregame\"],model = \"reddit\")\n",
    "lexicon.create_category(\"workout\", [\"workout\",\"rap\"],model = \"reddit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "track_name = data['playlists'][0]['tracks'][0]['track_name']\n",
    "artist_name = data['playlists'][0]['tracks'][0]['artist_name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "timeout=20\n",
    "import numpy as np\n",
    "emotions_df = pd.DataFrame()\n",
    "for i in range(len(data['playlists'][0]['tracks'])):\n",
    "    track_name = data['playlists'][0]['tracks'][i]['track_name']\n",
    "    artist_name = data['playlists'][0]['tracks'][i]['artist_name']\n",
    "    try: \n",
    "        song = genius.search_song(track_name, artist=artist_name, get_full_info = False)\n",
    "        if(song != None):\n",
    "            lyrics = song.lyrics\n",
    "\n",
    "            # Use the Empath object to analyze the text for different emotions\n",
    "            emotions = lexicon.analyze(lyrics, normalize=True)\n",
    "            emotions_df_song = pd.DataFrame(emotions, index=[0])\n",
    "\n",
    "            emotions_df = pd.concat([emotions_df, emotions_df_song], ignore_index = True)\n",
    "    except requests.exceptions.Timeout:\n",
    "        cats = list(lexicon.cats.keys())\n",
    "        nans = []\n",
    "        for i in range(len(cats)):\n",
    "            nans.append(np.nan)\n",
    "        nones = new_row = pd.Series(nans, index=cats)\n",
    "        emotions_df = pd.concat([emotions_df, emotions_df_song], ignore_index = True)\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine both togehter\n",
    "Put the audio features and the emotional features together into one LARGE dataframe!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "playlist_data = pd.concat([emotions_df, playlist_0],axis=1, ignore_index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"df\"+ \"0\" + \".csv\"\n",
    "directory = 'playlist_data'\n",
    "filename = os.path.join(directory, name)\n",
    "playlist_data.to_csv(filename, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we'll need to eventually store this as a .csv file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Time!\n",
    "\n",
    "The data we have about each playlist:\n",
    "1. The audio &  emotion values => playlist_data is for one playlist\n",
    "\n",
    "What we are trying to predit:\n",
    "1. The name => playlist_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataset object\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "class CustomImageDataset(tf.data.Dataset):\n",
    "    def __init__(self, names_file, playlist_data_dir):\n",
    "        self.name_labels = pd.read_csv(names_file) # might change\n",
    "        self.playlist_data_dir = playlist_data_dir # might change\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.name_labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # we want to get the dataframe correlated to this playlist\n",
    "        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])\n",
    "        file = self.playlist_data_dir + \"df\" + idx + \".csv\"\n",
    "        playlist_data = pd.read_csv(file)\n",
    "        label = self.name_labels.iloc[idx, 1] # get the label for this \n",
    "        return playlist_data, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'training_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[95], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataLoader\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Preparing your data for training with DataLoaders\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m train_dataloader \u001b[38;5;241m=\u001b[39m DataLoader(\u001b[43mtraining_data\u001b[49m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      5\u001b[0m test_dataloader \u001b[38;5;241m=\u001b[39m DataLoader(test_data, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'training_data' is not defined"
     ]
    }
   ],
   "source": [
    "# Preparing your data for training with DataLoaders\n",
    "train_dataloader = DataLoader(training_data, batch_size=64, shuffle=True)\n",
    "test_dataloader = DataLoader(test_data, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "10000\n",
      "11000\n",
      "12000\n",
      "13000\n",
      "14000\n",
      "15000\n",
      "16000\n",
      "17000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[105], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m high \u001b[38;5;241m=\u001b[39m high \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1000\u001b[39m\n\u001b[1;32m      8\u001b[0m f1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(file_name)\n\u001b[0;32m----> 9\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf1\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(low)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/json/__init__.py:293\u001b[0m, in \u001b[0;36mload\u001b[0;34m(fp, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    274\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload\u001b[39m(fp, \u001b[39m*\u001b[39m, \u001b[39mcls\u001b[39m\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, object_hook\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, parse_float\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    275\u001b[0m         parse_int\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, parse_constant\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, object_pairs_hook\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw):\n\u001b[1;32m    276\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Deserialize ``fp`` (a ``.read()``-supporting file-like object containing\u001b[39;00m\n\u001b[1;32m    277\u001b[0m \u001b[39m    a JSON document) to a Python object.\u001b[39;00m\n\u001b[1;32m    278\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    291\u001b[0m \u001b[39m    kwarg; otherwise ``JSONDecoder`` is used.\u001b[39;00m\n\u001b[1;32m    292\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 293\u001b[0m     \u001b[39mreturn\u001b[39;00m loads(fp\u001b[39m.\u001b[39;49mread(),\n\u001b[1;32m    294\u001b[0m         \u001b[39mcls\u001b[39;49m\u001b[39m=\u001b[39;49m\u001b[39mcls\u001b[39;49m, object_hook\u001b[39m=\u001b[39;49mobject_hook,\n\u001b[1;32m    295\u001b[0m         parse_float\u001b[39m=\u001b[39;49mparse_float, parse_int\u001b[39m=\u001b[39;49mparse_int,\n\u001b[1;32m    296\u001b[0m         parse_constant\u001b[39m=\u001b[39;49mparse_constant, object_pairs_hook\u001b[39m=\u001b[39;49mobject_pairs_hook, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkw)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/json/__init__.py:346\u001b[0m, in \u001b[0;36mloads\u001b[0;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    341\u001b[0m     s \u001b[39m=\u001b[39m s\u001b[39m.\u001b[39mdecode(detect_encoding(s), \u001b[39m'\u001b[39m\u001b[39msurrogatepass\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    343\u001b[0m \u001b[39mif\u001b[39;00m (\u001b[39mcls\u001b[39m \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m object_hook \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m\n\u001b[1;32m    344\u001b[0m         parse_int \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m parse_float \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m\n\u001b[1;32m    345\u001b[0m         parse_constant \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m object_pairs_hook \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m kw):\n\u001b[0;32m--> 346\u001b[0m     \u001b[39mreturn\u001b[39;00m _default_decoder\u001b[39m.\u001b[39;49mdecode(s)\n\u001b[1;32m    347\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mcls\u001b[39m \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    348\u001b[0m     \u001b[39mcls\u001b[39m \u001b[39m=\u001b[39m JSONDecoder\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/json/decoder.py:337\u001b[0m, in \u001b[0;36mJSONDecoder.decode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    332\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecode\u001b[39m(\u001b[39mself\u001b[39m, s, _w\u001b[39m=\u001b[39mWHITESPACE\u001b[39m.\u001b[39mmatch):\n\u001b[1;32m    333\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001b[39;00m\n\u001b[1;32m    334\u001b[0m \u001b[39m    containing a JSON document).\u001b[39;00m\n\u001b[1;32m    335\u001b[0m \n\u001b[1;32m    336\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 337\u001b[0m     obj, end \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mraw_decode(s, idx\u001b[39m=\u001b[39;49m_w(s, \u001b[39m0\u001b[39;49m)\u001b[39m.\u001b[39;49mend())\n\u001b[1;32m    338\u001b[0m     end \u001b[39m=\u001b[39m _w(s, end)\u001b[39m.\u001b[39mend()\n\u001b[1;32m    339\u001b[0m     \u001b[39mif\u001b[39;00m end \u001b[39m!=\u001b[39m \u001b[39mlen\u001b[39m(s):\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/json/decoder.py:353\u001b[0m, in \u001b[0;36mJSONDecoder.raw_decode\u001b[0;34m(self, s, idx)\u001b[0m\n\u001b[1;32m    344\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Decode a JSON document from ``s`` (a ``str`` beginning with\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[39ma JSON document) and return a 2-tuple of the Python\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[39mrepresentation and the index in ``s`` where the document ended.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    350\u001b[0m \n\u001b[1;32m    351\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    352\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 353\u001b[0m     obj, end \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mscan_once(s, idx)\n\u001b[1;32m    354\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[1;32m    355\u001b[0m     \u001b[39mraise\u001b[39;00m JSONDecodeError(\u001b[39m\"\u001b[39m\u001b[39mExpecting value\u001b[39m\u001b[39m\"\u001b[39m, s, err\u001b[39m.\u001b[39mvalue) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "max = 999000\n",
    "low = 0\n",
    "high = 999\n",
    "while low <= max:\n",
    "    file_name = \"data/mpd.slice.\" + str(low) + \"-\" + str(high) + \".json\"\n",
    "    low = low + 1000\n",
    "    high = high + 1000\n",
    "    f1 = open(file_name)\n",
    "    data = json.load(f1)\n",
    "    print(low)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.11 64-bit ('cse217a')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e27b9380ff38cc7a9bd155aef51d2581199facd5a3adc549696474b8eacd75fb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
